I0107 18:51:37.054662 140113887524864 llama.py:44] M: 0
N: 0
accumulation_steps: 1
adam_beta1: 0.9
adam_beta2: 0.95
adam_eps: 1.0e-08
batch_size: 8
cache_dir: null
epochs: 5
eval_zero_shot: false
infer_batch_size: 1
learning_rate: 0.0002
lr_scheduler: linear
max_grad_norm: 1000.0
model: meta-llama/Llama-2-7b-hf
nsamples: 256
prune_method: sparsegpt
seed: 0
self_nsamples: 0
seqlen: 1024
sparsity_ratio: 0.5
sparsity_type: unstructured
use_cr: true
use_fp32: true
use_gp: false
warmup_steps: 0
weight_decay: 0.0

I0107 18:51:37.057324 140113887524864 llama.py:54] loading llm model meta-llama/Llama-2-7b-hf
I0107 18:51:38.680388 140113887524864 llama.py:62] use device cuda:0
I0107 18:51:38.680571 140113887524864 llama.py:64] pruning starts
I0107 18:52:10.369118 140113887524864 prune.py:196] pruning layer 0
I0107 18:54:41.942436 140113887524864 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 18:55:49.345457 140113887524864 finetune.py:122] [1.96820474229753e-06, 2.3408792912960052e-06, 1.462642103433609e-06, 1.3818498700857162e-06, 1.3599637895822525e-06, 1.3480894267559052e-06]
I0107 18:55:53.179600 140113887524864 prune.py:200] pruning layer 0 & 1
I0107 19:00:30.885459 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 19:02:20.280543 140113887524864 finetune.py:122] [0.00014184322208166122, 0.03110705316066742, 0.03245067596435547, 0.00904393196105957, 0.002639293670654297, 0.0006363987922668457]
I0107 19:02:31.809324 140113887524864 prune.py:324] recon error 6.318185478448868e-05
I0107 19:02:32.480025 140113887524864 prune.py:200] pruning layer 1 & 2
I0107 19:07:10.463257 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 19:09:01.205957 140113887524864 finetune.py:122] [0.00014736130833625793, 0.0025599300861358643, 0.0005423128604888916, 0.0008656978607177734, 0.00030590593814849854, 0.00022698938846588135]
I0107 19:09:13.968919 140113887524864 prune.py:324] recon error 0.0005695205181837082
I0107 19:09:14.382934 140113887524864 prune.py:200] pruning layer 2 & 3
I0107 19:13:54.519646 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 19:15:41.810822 140113887524864 finetune.py:122] [0.00020753592252731323, 0.0001811683177947998, 0.00031068921089172363, 0.00017783045768737793, 0.00015395879745483398, 0.0001414269208908081]
I0107 19:15:54.991009 140113887524864 prune.py:324] recon error 0.0006914250552654266
I0107 19:15:55.439483 140113887524864 prune.py:200] pruning layer 3 & 4
I0107 19:20:41.576778 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 19:22:29.962317 140113887524864 finetune.py:122] [0.0003330931067466736, 0.00035387277603149414, 0.00039887428283691406, 0.00032842159271240234, 0.00027623772621154785, 0.0002536773681640625]
I0107 19:22:41.089831 140113887524864 prune.py:324] recon error 0.0009454488754272461
I0107 19:22:41.451908 140113887524864 prune.py:200] pruning layer 4 & 5
I0107 19:27:22.768495 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 19:29:10.119544 140113887524864 finetune.py:122] [0.0005176812410354614, 0.0005688667297363281, 0.0008238554000854492, 0.0005509257316589355, 0.0004540085792541504, 0.00040847063064575195]
I0107 19:29:24.733865 140113887524864 prune.py:324] recon error 0.0014129877090454102
I0107 19:29:25.296545 140113887524864 prune.py:200] pruning layer 5 & 6
I0107 19:34:09.861759 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 19:35:58.634130 140113887524864 finetune.py:122] [0.0006970316171646118, 0.000970005989074707, 0.0014014244079589844, 0.0010124444961547852, 0.0007932186126708984, 0.0006899833679199219]
I0107 19:36:12.244662 140113887524864 prune.py:324] recon error 0.002140641212463379
I0107 19:36:12.664524 140113887524864 prune.py:200] pruning layer 6 & 7
I0107 19:40:55.582256 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 19:42:44.794528 140113887524864 finetune.py:122] [0.0009234249591827393, 0.0011545419692993164, 0.0014026165008544922, 0.0011020898818969727, 0.0008742809295654297, 0.0007700920104980469]
I0107 19:42:56.816340 140113887524864 prune.py:324] recon error 0.003087669610977173
I0107 19:42:57.220915 140113887524864 prune.py:200] pruning layer 7 & 8
I0107 19:47:49.588891 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 19:49:42.087898 140113887524864 finetune.py:122] [0.0011854320764541626, 0.0014963150024414062, 0.0017862319946289062, 0.001445770263671875, 0.0011156797409057617, 0.0009762048721313477]
I0107 19:49:53.191084 140113887524864 prune.py:324] recon error 0.004232347011566162
I0107 19:49:53.580828 140113887524864 prune.py:200] pruning layer 8 & 9
I0107 19:54:44.281632 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 19:56:33.682385 140113887524864 finetune.py:122] [0.001531451940536499, 0.0018768310546875, 0.002184152603149414, 0.001668691635131836, 0.0013301372528076172, 0.0011742115020751953]
I0107 19:56:44.454803 140113887524864 prune.py:324] recon error 0.005784034729003906
I0107 19:56:44.918043 140113887524864 prune.py:200] pruning layer 9 & 10
I0107 20:01:32.450612 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 20:03:20.402613 140113887524864 finetune.py:122] [0.001834571361541748, 0.0023114681243896484, 0.0027327537536621094, 0.0020911693572998047, 0.0016317367553710938, 0.0014226436614990234]
I0107 20:03:30.389023 140113887524864 prune.py:324] recon error 0.007724642753601074
I0107 20:03:30.874207 140113887524864 prune.py:200] pruning layer 10 & 11
I0107 20:08:11.541305 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 20:09:57.332370 140113887524864 finetune.py:122] [0.001994699239730835, 0.0027670860290527344, 0.003276824951171875, 0.0024526119232177734, 0.0018749237060546875, 0.001607656478881836]
I0107 20:10:08.818538 140113887524864 prune.py:324] recon error 0.009800910949707031
I0107 20:10:09.463973 140113887524864 prune.py:200] pruning layer 11 & 12
I0107 20:14:44.392088 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 20:16:32.510661 140113887524864 finetune.py:122] [0.0023429393768310547, 0.0027222633361816406, 0.003627777099609375, 0.002663135528564453, 0.0020906925201416016, 0.0018072128295898438]
I0107 20:16:45.019365 140113887524864 prune.py:324] recon error 0.011785030364990234
I0107 20:16:45.635638 140113887524864 prune.py:200] pruning layer 12 & 13
I0107 20:21:28.713131 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 20:23:15.664448 140113887524864 finetune.py:122] [0.0028529465198516846, 0.0035104751586914062, 0.003910541534423828, 0.003005504608154297, 0.0023407936096191406, 0.002048492431640625]
I0107 20:23:27.149818 140113887524864 prune.py:324] recon error 0.014131784439086914
I0107 20:23:27.840914 140113887524864 prune.py:200] pruning layer 13 & 14
I0107 20:28:07.029103 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 20:29:54.971864 140113887524864 finetune.py:122] [0.003099292516708374, 0.0036559104919433594, 0.004413604736328125, 0.003489971160888672, 0.0027370452880859375, 0.0023679733276367188]
I0107 20:30:07.316957 140113887524864 prune.py:324] recon error 0.016893625259399414
I0107 20:30:07.710363 140113887524864 prune.py:200] pruning layer 14 & 15
I0107 20:34:56.024980 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 20:36:44.490213 140113887524864 finetune.py:122] [0.0038452744483947754, 0.004425525665283203, 0.004857063293457031, 0.003806591033935547, 0.0030660629272460938, 0.0027322769165039062]
I0107 20:36:56.087260 140113887524864 prune.py:324] recon error 0.020038604736328125
I0107 20:36:56.493962 140113887524864 prune.py:200] pruning layer 15 & 16
I0107 20:41:32.352986 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 20:43:16.982375 140113887524864 finetune.py:122] [0.005079209804534912, 0.0064563751220703125, 0.008562088012695312, 0.00612640380859375, 0.004647731781005859, 0.003936767578125]
I0107 20:43:26.579986 140113887524864 prune.py:324] recon error 0.02443099021911621
I0107 20:43:27.017435 140113887524864 prune.py:200] pruning layer 16 & 17
I0107 20:47:54.210188 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 20:49:36.608414 140113887524864 finetune.py:122] [0.00531846284866333, 0.006125450134277344, 0.009774208068847656, 0.0065441131591796875, 0.005049705505371094, 0.004263401031494141]
I0107 20:49:45.680169 140113887524864 prune.py:324] recon error 0.03233051300048828
I0107 20:49:46.093421 140113887524864 prune.py:200] pruning layer 17 & 18
I0107 20:54:12.433179 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 20:55:57.386142 140113887524864 finetune.py:122] [0.006297647953033447, 0.006928443908691406, 0.014853477478027344, 0.0068721771240234375, 0.005471229553222656, 0.004670143127441406]
I0107 20:56:07.036724 140113887524864 prune.py:324] recon error 0.03952932357788086
I0107 20:56:07.453775 140113887524864 prune.py:200] pruning layer 18 & 19
I0107 21:00:35.953934 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 21:02:17.769437 140113887524864 finetune.py:122] [0.006849050521850586, 0.007175445556640625, 0.008938789367675781, 0.007155418395996094, 0.005791664123535156, 0.0049686431884765625]
I0107 21:02:26.781002 140113887524864 prune.py:324] recon error 0.050693511962890625
I0107 21:02:27.208385 140113887524864 prune.py:200] pruning layer 19 & 20
I0107 21:06:52.607303 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 21:08:35.551252 140113887524864 finetune.py:122] [0.008237957954406738, 0.009752273559570312, 0.016096115112304688, 0.010213851928710938, 0.007748603820800781, 0.006443023681640625]
I0107 21:08:44.247616 140113887524864 prune.py:324] recon error 0.06409168243408203
I0107 21:08:44.660922 140113887524864 prune.py:200] pruning layer 20 & 21
I0107 21:13:10.062376 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 21:14:52.240579 140113887524864 finetune.py:122] [0.00820004940032959, 0.008819580078125, 0.013628005981445312, 0.010224342346191406, 0.0075550079345703125, 0.006394386291503906]
I0107 21:15:01.490514 140113887524864 prune.py:324] recon error 0.0838613510131836
I0107 21:15:01.893399 140113887524864 prune.py:200] pruning layer 21 & 22
I0107 21:19:26.902209 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 21:21:08.782869 140113887524864 finetune.py:122] [0.009951114654541016, 0.010593414306640625, 0.01544189453125, 0.011064529418945312, 0.008748054504394531, 0.007405281066894531]
I0107 21:21:18.440486 140113887524864 prune.py:324] recon error 0.10225105285644531
I0107 21:21:18.840104 140113887524864 prune.py:200] pruning layer 22 & 23
I0107 21:25:46.006870 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 21:27:30.721550 140113887524864 finetune.py:122] [0.010712146759033203, 0.010387420654296875, 0.01425933837890625, 0.0103302001953125, 0.008820533752441406, 0.00748443603515625]
I0107 21:27:39.319619 140113887524864 prune.py:324] recon error 0.13085174560546875
I0107 21:27:39.722544 140113887524864 prune.py:200] pruning layer 23 & 24
I0107 21:32:03.447547 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 21:33:46.158208 140113887524864 finetune.py:122] [0.011648058891296387, 0.011501312255859375, 0.016450881958007812, 0.010974884033203125, 0.00897979736328125, 0.007975578308105469]
I0107 21:33:54.671568 140113887524864 prune.py:324] recon error 0.15718650817871094
I0107 21:33:55.067820 140113887524864 prune.py:200] pruning layer 24 & 25
I0107 21:38:22.671923 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 21:40:04.125332 140113887524864 finetune.py:122] [0.011436700820922852, 0.011562347412109375, 0.017301559448242188, 0.011182785034179688, 0.009298324584960938, 0.008212089538574219]
I0107 21:40:12.507852 140113887524864 prune.py:324] recon error 0.1869487762451172
I0107 21:40:12.929320 140113887524864 prune.py:200] pruning layer 25 & 26
I0107 21:44:36.706416 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 21:46:20.342060 140113887524864 finetune.py:122] [0.01443946361541748, 0.014057159423828125, 0.019033432006835938, 0.012998580932617188, 0.010931015014648438, 0.009737014770507812]
I0107 21:46:29.811916 140113887524864 prune.py:324] recon error 0.21781539916992188
I0107 21:46:30.216587 140113887524864 prune.py:200] pruning layer 26 & 27
I0107 21:50:57.087045 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 21:52:39.947459 140113887524864 finetune.py:122] [0.015660405158996582, 0.014810562133789062, 0.019182205200195312, 0.0136566162109375, 0.01140594482421875, 0.01019287109375]
I0107 21:52:50.215926 140113887524864 prune.py:324] recon error 0.2610282897949219
I0107 21:52:50.638119 140113887524864 prune.py:200] pruning layer 27 & 28
I0107 21:57:18.070141 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 21:59:02.012701 140113887524864 finetune.py:122] [0.018922805786132812, 0.0177764892578125, 0.0244903564453125, 0.01686859130859375, 0.013811111450195312, 0.01238250732421875]
I0107 21:59:11.496839 140113887524864 prune.py:324] recon error 0.3042411804199219
I0107 21:59:11.894698 140113887524864 prune.py:200] pruning layer 28 & 29
I0107 22:03:41.927724 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 22:05:24.758234 140113887524864 finetune.py:122] [0.021107912063598633, 0.02056121826171875, 0.03046417236328125, 0.01988983154296875, 0.01667022705078125, 0.014394760131835938]
I0107 22:05:34.241549 140113887524864 prune.py:324] recon error 0.36418914794921875
I0107 22:05:34.638880 140113887524864 prune.py:200] pruning layer 29 & 30
I0107 22:10:00.686557 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 22:11:43.463439 140113887524864 finetune.py:122] [0.032998085021972656, 0.03107452392578125, 0.046253204345703125, 0.02761077880859375, 0.02204132080078125, 0.0191650390625]
I0107 22:11:52.703939 140113887524864 prune.py:324] recon error 0.4255867004394531
I0107 22:11:53.112110 140113887524864 prune.py:200] pruning layer 30 & 31
I0107 22:16:21.054447 140113887524864 finetune.py:128] weight params, number of params: 404766720, weight_decay: 0.0, lr: 0.0002
I0107 22:18:03.951205 140113887524864 finetune.py:122] [0.050423622131347656, 0.1641387939453125, 0.240570068359375, 0.1080169677734375, 0.074981689453125, 0.05426025390625]
I0107 22:18:13.377720 140113887524864 prune.py:324] recon error 0.5353622436523438
I0107 22:18:13.773081 140113887524864 prune.py:198] pruning layer 31
I0107 22:20:33.601581 140113887524864 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 22:21:28.183897 140113887524864 finetune.py:122] [0.00012231990694999695, 0.010114073753356934, 0.021327972412109375, 0.008098602294921875, 0.004099369049072266, 0.00186920166015625]
I0107 22:21:36.397310 140113887524864 prune.py:324] recon error 0.9676437377929688
I0107 22:21:36.600611 140113887524864 llama.py:71] ******************************
I0107 22:21:36.692954 140113887524864 prune.py:59] block 0 sparsity 0.500002
I0107 22:21:36.787226 140113887524864 prune.py:59] block 1 sparsity 0.500002
I0107 22:21:36.880510 140113887524864 prune.py:59] block 2 sparsity 0.500002
I0107 22:21:36.977382 140113887524864 prune.py:59] block 3 sparsity 0.500002
I0107 22:21:37.076455 140113887524864 prune.py:59] block 4 sparsity 0.500002
I0107 22:21:37.170413 140113887524864 prune.py:59] block 5 sparsity 0.500002
I0107 22:21:37.267846 140113887524864 prune.py:59] block 6 sparsity 0.500002
I0107 22:21:37.366583 140113887524864 prune.py:59] block 7 sparsity 0.500002
I0107 22:21:37.453760 140113887524864 prune.py:59] block 8 sparsity 0.500002
I0107 22:21:37.545606 140113887524864 prune.py:59] block 9 sparsity 0.500002
I0107 22:21:37.629392 140113887524864 prune.py:59] block 10 sparsity 0.500001
I0107 22:21:37.717073 140113887524864 prune.py:59] block 11 sparsity 0.500002
I0107 22:21:37.803276 140113887524864 prune.py:59] block 12 sparsity 0.500002
I0107 22:21:37.887209 140113887524864 prune.py:59] block 13 sparsity 0.500002
I0107 22:21:37.971707 140113887524864 prune.py:59] block 14 sparsity 0.500002
I0107 22:21:38.055549 140113887524864 prune.py:59] block 15 sparsity 0.500002
I0107 22:21:38.139452 140113887524864 prune.py:59] block 16 sparsity 0.500002
I0107 22:21:38.223151 140113887524864 prune.py:59] block 17 sparsity 0.500002
I0107 22:21:38.307582 140113887524864 prune.py:59] block 18 sparsity 0.500002
I0107 22:21:38.391357 140113887524864 prune.py:59] block 19 sparsity 0.500002
I0107 22:21:38.476772 140113887524864 prune.py:59] block 20 sparsity 0.500002
I0107 22:21:38.560461 140113887524864 prune.py:59] block 21 sparsity 0.500002
I0107 22:21:38.645268 140113887524864 prune.py:59] block 22 sparsity 0.500002
I0107 22:21:38.729643 140113887524864 prune.py:59] block 23 sparsity 0.500002
I0107 22:21:38.815318 140113887524864 prune.py:59] block 24 sparsity 0.500002
I0107 22:21:38.900123 140113887524864 prune.py:59] block 25 sparsity 0.500002
I0107 22:21:38.983556 140113887524864 prune.py:59] block 26 sparsity 0.500002
I0107 22:21:39.066738 140113887524864 prune.py:59] block 27 sparsity 0.500002
I0107 22:21:39.151209 140113887524864 prune.py:59] block 28 sparsity 0.500002
I0107 22:21:39.235753 140113887524864 prune.py:59] block 29 sparsity 0.500002
I0107 22:21:39.299917 140113887524864 prune.py:59] block 30 sparsity 0.500002
I0107 22:21:39.364022 140113887524864 prune.py:59] block 31 sparsity 0.500002
I0107 22:21:39.364196 140113887524864 llama.py:73] sparsity sanity check 0.5000
I0107 22:21:39.364237 140113887524864 llama.py:74] ******************************
I0107 22:21:44.311418 140113887524864 eval.py:26] evaluating on wikitext2
I0107 22:22:40.484252 140113887524864 eval.py:48] nsamples 83
I0107 22:22:40.484436 140113887524864 eval.py:53] sample 0
I0107 22:23:21.267583 140113887524864 eval.py:53] sample 50
I0107 22:23:48.190014 140113887524864 eval.py:18] wikitext2 perplexity 6.617672920227051
I0107 22:23:48.190171 140113887524864 eval.py:26] evaluating on ptb
I0107 22:24:01.567940 140113887524864 eval.py:48] nsamples 24
I0107 22:24:01.568104 140113887524864 eval.py:53] sample 0
I0107 22:24:21.146044 140113887524864 eval.py:18] ptb perplexity 342.9345703125
I0107 22:24:21.146200 140113887524864 eval.py:26] evaluating on c4
I0107 22:25:19.199391 140113887524864 eval.py:48] nsamples 149
I0107 22:25:19.199567 140113887524864 eval.py:53] sample 0
I0107 22:25:59.967258 140113887524864 eval.py:53] sample 50
I0107 22:26:40.747893 140113887524864 eval.py:53] sample 100
I0107 22:27:20.696244 140113887524864 eval.py:18] c4 perplexity 8.876529693603516
