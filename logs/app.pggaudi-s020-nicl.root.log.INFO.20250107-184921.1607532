I0107 18:49:21.422254 140159186819072 llama.py:44] M: 0
N: 0
accumulation_steps: 1
adam_beta1: 0.9
adam_beta2: 0.95
adam_eps: 1.0e-08
batch_size: 8
cache_dir: null
epochs: 5
eval_zero_shot: false
infer_batch_size: 1
learning_rate: 0.0002
lr_scheduler: linear
max_grad_norm: 1000.0
model: meta-llama/Llama-2-7b-hf
nsamples: 256
prune_method: wanda
seed: 0
self_nsamples: 0
seqlen: 1024
sparsity_ratio: 0.5
sparsity_type: unstructured
use_cr: false
use_fp32: true
use_gp: false
warmup_steps: 0
weight_decay: 0.0

I0107 18:49:21.425004 140159186819072 llama.py:54] loading llm model meta-llama/Llama-2-7b-hf
I0107 18:49:23.110759 140159186819072 llama.py:62] use device cuda:0
I0107 18:49:23.110908 140159186819072 llama.py:64] pruning starts
I0107 18:49:54.341305 140159186819072 prune.py:202] pruning layer 0
I0107 18:50:05.801055 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 18:51:07.051114 140159186819072 finetune.py:122] [5.351612344384193e-06, 3.7886202335357666e-06, 2.2388994693756104e-06, 2.107582986354828e-06, 2.0693987607955933e-06, 2.0496081560850143e-06]
I0107 18:51:12.454203 140159186819072 prune.py:292] recon error 2.044951543211937e-06
I0107 18:51:15.907204 140159186819072 prune.py:202] pruning layer 1
I0107 18:51:29.370714 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 18:52:33.467512 140159186819072 finetune.py:122] [0.018199443817138672, 0.009180068969726562, 0.010477066040039062, 0.003773927688598633, 0.002325773239135742, 0.0008473694324493408]
I0107 18:52:38.290045 140159186819072 prune.py:292] recon error 0.0008076298981904984
I0107 18:52:41.896239 140159186819072 prune.py:202] pruning layer 2
I0107 18:52:55.091106 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 18:53:53.568596 140159186819072 finetune.py:122] [0.00011873245239257812, 7.91698694229126e-05, 7.269531488418579e-05, 6.36950135231018e-05, 6.103515625e-05, 5.984306335449219e-05]
I0107 18:53:57.764890 140159186819072 prune.py:292] recon error 0.0009290650486946106
I0107 18:54:01.829619 140159186819072 prune.py:202] pruning layer 3
I0107 18:54:15.353718 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 18:55:14.598286 140159186819072 finetune.py:122] [0.00028686225414276123, 0.00018793344497680664, 0.0001703798770904541, 0.00015142560005187988, 0.00014191865921020508, 0.0001360476016998291]
I0107 18:55:18.724167 140159186819072 prune.py:292] recon error 0.0011527463793754578
I0107 18:55:22.654482 140159186819072 prune.py:202] pruning layer 4
I0107 18:55:34.420828 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 18:56:32.516514 140159186819072 finetune.py:122] [0.0004628673195838928, 0.0003387331962585449, 0.0003134608268737793, 0.00027298927307128906, 0.0002499222755432129, 0.0002410411834716797]
I0107 18:56:35.682195 140159186819072 prune.py:292] recon error 0.0015477538108825684
I0107 18:56:38.897311 140159186819072 prune.py:202] pruning layer 5
I0107 18:56:51.062583 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 18:57:48.592282 140159186819072 finetune.py:122] [0.000668749213218689, 0.000518798828125, 0.00047832727432250977, 0.0004150867462158203, 0.0003763437271118164, 0.0003593564033508301]
I0107 18:57:51.811128 140159186819072 prune.py:292] recon error 0.002093285322189331
I0107 18:57:55.001730 140159186819072 prune.py:202] pruning layer 6
I0107 18:58:06.315520 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 18:59:05.001893 140159186819072 finetune.py:122] [0.0010083168745040894, 0.00078582763671875, 0.0008053779602050781, 0.0006657838821411133, 0.0005826950073242188, 0.0005456209182739258]
I0107 18:59:08.288273 140159186819072 prune.py:292] recon error 0.0028418004512786865
I0107 18:59:11.169746 140159186819072 prune.py:202] pruning layer 7
I0107 18:59:22.334643 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:00:20.390498 140159186819072 finetune.py:122] [0.0012787282466888428, 0.0010505914688110352, 0.0009816884994506836, 0.0008347034454345703, 0.0007393360137939453, 0.0006996393203735352]
I0107 19:00:24.715321 140159186819072 prune.py:292] recon error 0.003753185272216797
I0107 19:00:28.072287 140159186819072 prune.py:202] pruning layer 8
I0107 19:00:39.769697 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:01:39.672763 140159186819072 finetune.py:122] [0.0016581714153289795, 0.001338958740234375, 0.0012667179107666016, 0.0010622739791870117, 0.0009456872940063477, 0.0008848905563354492]
I0107 19:01:42.828152 140159186819072 prune.py:292] recon error 0.004942357540130615
I0107 19:01:46.146315 140159186819072 prune.py:202] pruning layer 9
I0107 19:01:57.345476 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:02:55.440927 140159186819072 finetune.py:122] [0.002092599868774414, 0.0016436576843261719, 0.0016207695007324219, 0.0013055801391601562, 0.0011494159698486328, 0.0010852813720703125]
I0107 19:02:58.475611 140159186819072 prune.py:292] recon error 0.006388545036315918
I0107 19:03:01.360628 140159186819072 prune.py:202] pruning layer 10
I0107 19:03:12.054446 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:04:09.556397 140159186819072 finetune.py:122] [0.0025870203971862793, 0.002053499221801758, 0.0018486976623535156, 0.00159454345703125, 0.0013980865478515625, 0.0013165473937988281]
I0107 19:04:12.709902 140159186819072 prune.py:292] recon error 0.008021235466003418
I0107 19:04:16.363452 140159186819072 prune.py:202] pruning layer 11
I0107 19:04:27.616023 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:05:26.333976 140159186819072 finetune.py:122] [0.002918064594268799, 0.0024552345275878906, 0.0021483898162841797, 0.0018014907836914062, 0.0015821456909179688, 0.0014853477478027344]
I0107 19:05:29.419636 140159186819072 prune.py:292] recon error 0.009601473808288574
I0107 19:05:32.328713 140159186819072 prune.py:202] pruning layer 12
I0107 19:05:43.624971 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:06:40.536038 140159186819072 finetune.py:122] [0.0032375454902648926, 0.0026335716247558594, 0.00373077392578125, 0.002033710479736328, 0.0017685890197753906, 0.0016613006591796875]
I0107 19:06:43.604624 140159186819072 prune.py:292] recon error 0.011501073837280273
I0107 19:06:46.625119 140159186819072 prune.py:202] pruning layer 13
I0107 19:06:57.529281 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:07:56.283472 140159186819072 finetune.py:122] [0.004065871238708496, 0.0031142234802246094, 0.002819538116455078, 0.0024309158325195312, 0.002112865447998047, 0.0019805431365966797]
I0107 19:07:59.699949 140159186819072 prune.py:292] recon error 0.013680696487426758
I0107 19:08:03.255199 140159186819072 prune.py:202] pruning layer 14
I0107 19:08:16.077326 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:09:16.965420 140159186819072 finetune.py:122] [0.004458606243133545, 0.0038013458251953125, 0.00513458251953125, 0.002857685089111328, 0.0025048255920410156, 0.0023627281188964844]
I0107 19:09:20.409869 140159186819072 prune.py:292] recon error 0.01633477210998535
I0107 19:09:23.674091 140159186819072 prune.py:202] pruning layer 15
I0107 19:09:37.147334 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:10:34.114967 140159186819072 finetune.py:122] [0.005475223064422607, 0.004342079162597656, 0.0036716461181640625, 0.003211498260498047, 0.002899646759033203, 0.0027480125427246094]
I0107 19:10:37.271340 140159186819072 prune.py:292] recon error 0.019710063934326172
I0107 19:10:40.475838 140159186819072 prune.py:202] pruning layer 16
I0107 19:10:51.891874 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:11:50.706124 140159186819072 finetune.py:122] [0.007502555847167969, 0.005984306335449219, 0.0052890777587890625, 0.004561424255371094, 0.003987789154052734, 0.0037689208984375]
I0107 19:11:53.831712 140159186819072 prune.py:292] recon error 0.025823116302490234
I0107 19:11:56.976917 140159186819072 prune.py:202] pruning layer 17
I0107 19:12:07.660479 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:13:04.831646 140159186819072 finetune.py:122] [0.0074340105056762695, 0.006201744079589844, 0.0069484710693359375, 0.005021095275878906, 0.004406929016113281, 0.0040760040283203125]
I0107 19:13:08.123175 140159186819072 prune.py:292] recon error 0.03145599365234375
I0107 19:13:11.039170 140159186819072 prune.py:202] pruning layer 18
I0107 19:13:22.364030 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:14:19.780516 140159186819072 finetune.py:122] [0.00879371166229248, 0.0070362091064453125, 0.006725311279296875, 0.005440711975097656, 0.004828453063964844, 0.0045909881591796875]
I0107 19:14:24.379848 140159186819072 prune.py:292] recon error 0.03995370864868164
I0107 19:14:27.643811 140159186819072 prune.py:202] pruning layer 19
I0107 19:14:39.151400 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:15:36.638864 140159186819072 finetune.py:122] [0.00953686237335205, 0.007781028747558594, 0.006915092468261719, 0.005936622619628906, 0.005339622497558594, 0.005096435546875]
I0107 19:15:39.680562 140159186819072 prune.py:292] recon error 0.05025005340576172
I0107 19:15:42.905589 140159186819072 prune.py:202] pruning layer 20
I0107 19:15:56.489723 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:16:54.531758 140159186819072 finetune.py:122] [0.011865615844726562, 0.009775161743164062, 0.010669708251953125, 0.007737159729003906, 0.00678253173828125, 0.006306648254394531]
I0107 19:16:57.552598 140159186819072 prune.py:292] recon error 0.06510353088378906
I0107 19:17:00.978923 140159186819072 prune.py:202] pruning layer 21
I0107 19:17:14.087942 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:18:12.458837 140159186819072 finetune.py:122] [0.01160895824432373, 0.009935379028320312, 0.009528160095214844, 0.007610321044921875, 0.006787300109863281, 0.0064258575439453125]
I0107 19:18:16.769063 140159186819072 prune.py:292] recon error 0.08001518249511719
I0107 19:18:19.739707 140159186819072 prune.py:202] pruning layer 22
I0107 19:18:30.513835 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:19:28.868835 140159186819072 finetune.py:122] [0.013579607009887695, 0.01137542724609375, 0.0110015869140625, 0.008782386779785156, 0.007825851440429688, 0.007390022277832031]
I0107 19:19:32.422662 140159186819072 prune.py:292] recon error 0.10070991516113281
I0107 19:19:35.709804 140159186819072 prune.py:202] pruning layer 23
I0107 19:19:46.885064 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:20:44.130857 140159186819072 finetune.py:122] [0.01413726806640625, 0.012159347534179688, 0.0119476318359375, 0.009334564208984375, 0.00846099853515625, 0.008016586303710938]
I0107 19:20:47.145256 140159186819072 prune.py:292] recon error 0.12082290649414062
I0107 19:20:50.620238 140159186819072 prune.py:202] pruning layer 24
I0107 19:21:01.934608 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:22:02.584978 140159186819072 finetune.py:122] [0.01585674285888672, 0.013406753540039062, 0.012477874755859375, 0.010061264038085938, 0.00905609130859375, 0.008626937866210938]
I0107 19:22:05.804860 140159186819072 prune.py:292] recon error 0.14551734924316406
I0107 19:22:10.028787 140159186819072 prune.py:202] pruning layer 25
I0107 19:22:22.196552 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:23:19.619465 140159186819072 finetune.py:122] [0.01556396484375, 0.013570785522460938, 0.012834548950195312, 0.010545730590820312, 0.009471893310546875, 0.00899505615234375]
I0107 19:23:23.754636 140159186819072 prune.py:292] recon error 0.1695404052734375
I0107 19:23:26.849454 140159186819072 prune.py:202] pruning layer 26
I0107 19:23:37.766479 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:24:35.473925 140159186819072 finetune.py:122] [0.01963973045349121, 0.016279220581054688, 0.015300750732421875, 0.012420654296875, 0.011241912841796875, 0.010530471801757812]
I0107 19:24:38.495887 140159186819072 prune.py:292] recon error 0.20334625244140625
I0107 19:24:41.899887 140159186819072 prune.py:202] pruning layer 27
I0107 19:24:52.771602 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:25:50.081229 140159186819072 finetune.py:122] [0.021207332611083984, 0.017259597778320312, 0.016265869140625, 0.013250350952148438, 0.011974334716796875, 0.011377334594726562]
I0107 19:25:53.071370 140159186819072 prune.py:292] recon error 0.23812103271484375
I0107 19:25:56.141946 140159186819072 prune.py:202] pruning layer 28
I0107 19:26:07.087142 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:27:05.564489 140159186819072 finetune.py:122] [0.02678823471069336, 0.02075958251953125, 0.021638870239257812, 0.016035079956054688, 0.014495849609375, 0.013519287109375]
I0107 19:27:08.526863 140159186819072 prune.py:292] recon error 0.2829475402832031
I0107 19:27:11.479722 140159186819072 prune.py:202] pruning layer 29
I0107 19:27:22.961709 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:28:21.242511 140159186819072 finetune.py:122] [0.030251502990722656, 0.023773193359375, 0.024532318115234375, 0.018419265747070312, 0.016412734985351562, 0.015445709228515625]
I0107 19:28:24.891751 140159186819072 prune.py:292] recon error 0.33586883544921875
I0107 19:28:28.127468 140159186819072 prune.py:202] pruning layer 30
I0107 19:28:39.855760 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:29:39.107729 140159186819072 finetune.py:122] [0.09215164184570312, 0.037014007568359375, 0.028743743896484375, 0.02370452880859375, 0.020961761474609375, 0.019519805908203125]
I0107 19:29:42.087843 140159186819072 prune.py:292] recon error 0.4143218994140625
I0107 19:29:46.455207 140159186819072 prune.py:202] pruning layer 31
I0107 19:29:57.354032 140159186819072 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:30:54.309576 140159186819072 finetune.py:122] [0.10564994812011719, 0.07889556884765625, 0.0828857421875, 0.05525970458984375, 0.04181671142578125, 0.03626251220703125]
I0107 19:30:57.256852 140159186819072 prune.py:292] recon error 0.720916748046875
I0107 19:31:00.239979 140159186819072 llama.py:71] ******************************
I0107 19:31:00.332140 140159186819072 prune.py:59] block 0 sparsity 0.500000
I0107 19:31:00.425703 140159186819072 prune.py:59] block 1 sparsity 0.500000
I0107 19:31:00.516571 140159186819072 prune.py:59] block 2 sparsity 0.500000
I0107 19:31:00.621251 140159186819072 prune.py:59] block 3 sparsity 0.500000
I0107 19:31:00.728333 140159186819072 prune.py:59] block 4 sparsity 0.500000
I0107 19:31:00.837965 140159186819072 prune.py:59] block 5 sparsity 0.500000
I0107 19:31:00.946556 140159186819072 prune.py:59] block 6 sparsity 0.500000
I0107 19:31:01.054053 140159186819072 prune.py:59] block 7 sparsity 0.500000
I0107 19:31:01.159017 140159186819072 prune.py:59] block 8 sparsity 0.500000
I0107 19:31:01.270563 140159186819072 prune.py:59] block 9 sparsity 0.500000
I0107 19:31:01.381347 140159186819072 prune.py:59] block 10 sparsity 0.500000
I0107 19:31:01.491794 140159186819072 prune.py:59] block 11 sparsity 0.500000
I0107 19:31:01.606268 140159186819072 prune.py:59] block 12 sparsity 0.500000
I0107 19:31:01.709299 140159186819072 prune.py:59] block 13 sparsity 0.500000
I0107 19:31:01.822699 140159186819072 prune.py:59] block 14 sparsity 0.500000
I0107 19:31:01.923379 140159186819072 prune.py:59] block 15 sparsity 0.500000
I0107 19:31:02.031522 140159186819072 prune.py:59] block 16 sparsity 0.500000
I0107 19:31:02.133395 140159186819072 prune.py:59] block 17 sparsity 0.500000
I0107 19:31:02.237495 140159186819072 prune.py:59] block 18 sparsity 0.500000
I0107 19:31:02.345351 140159186819072 prune.py:59] block 19 sparsity 0.500000
I0107 19:31:02.458787 140159186819072 prune.py:59] block 20 sparsity 0.500000
I0107 19:31:02.568776 140159186819072 prune.py:59] block 21 sparsity 0.500000
I0107 19:31:02.680839 140159186819072 prune.py:59] block 22 sparsity 0.500000
I0107 19:31:02.779181 140159186819072 prune.py:59] block 23 sparsity 0.500000
I0107 19:31:02.884973 140159186819072 prune.py:59] block 24 sparsity 0.500000
I0107 19:31:02.997334 140159186819072 prune.py:59] block 25 sparsity 0.500000
I0107 19:31:03.094685 140159186819072 prune.py:59] block 26 sparsity 0.500000
I0107 19:31:03.200906 140159186819072 prune.py:59] block 27 sparsity 0.500000
I0107 19:31:03.311302 140159186819072 prune.py:59] block 28 sparsity 0.500000
I0107 19:31:03.420761 140159186819072 prune.py:59] block 29 sparsity 0.500000
I0107 19:31:03.495865 140159186819072 prune.py:59] block 30 sparsity 0.500000
I0107 19:31:03.557673 140159186819072 prune.py:59] block 31 sparsity 0.500000
I0107 19:31:03.557871 140159186819072 llama.py:73] sparsity sanity check 0.5000
I0107 19:31:03.557915 140159186819072 llama.py:74] ******************************
I0107 19:31:08.844368 140159186819072 eval.py:26] evaluating on wikitext2
I0107 19:32:10.875389 140159186819072 eval.py:48] nsamples 83
I0107 19:32:10.875573 140159186819072 eval.py:53] sample 0
I0107 19:32:50.156688 140159186819072 eval.py:53] sample 50
I0107 19:33:16.073543 140159186819072 eval.py:18] wikitext2 perplexity 6.261020660400391
I0107 19:33:16.073705 140159186819072 eval.py:26] evaluating on ptb
I0107 19:33:29.971155 140159186819072 eval.py:48] nsamples 24
I0107 19:33:29.971311 140159186819072 eval.py:53] sample 0
I0107 19:33:48.824625 140159186819072 eval.py:18] ptb perplexity 904.6412963867188
I0107 19:33:48.824784 140159186819072 eval.py:26] evaluating on c4
I0107 19:34:52.031852 140159186819072 eval.py:48] nsamples 149
I0107 19:34:52.032026 140159186819072 eval.py:53] sample 0
I0107 19:35:31.326783 140159186819072 eval.py:53] sample 50
I0107 19:36:10.636306 140159186819072 eval.py:53] sample 100
I0107 19:36:49.117774 140159186819072 eval.py:18] c4 perplexity 8.432113647460938
