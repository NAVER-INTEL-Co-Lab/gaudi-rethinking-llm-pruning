I0107 19:08:32.630282 140131859355648 llama.py:44] M: 0
N: 0
accumulation_steps: 1
adam_beta1: 0.9
adam_beta2: 0.95
adam_eps: 1.0e-08
batch_size: 8
cache_dir: null
epochs: 5
eval_zero_shot: false
infer_batch_size: 1
learning_rate: 0.0002
lr_scheduler: linear
max_grad_norm: 1000.0
model: meta-llama/Llama-2-7b-hf
nsamples: 256
prune_method: wanda
seed: 0
self_nsamples: 0
seqlen: 1024
sparsity_ratio: 0.5
sparsity_type: unstructured
use_cr: false
use_fp32: true
use_gp: false
warmup_steps: 0
weight_decay: 0.0

I0107 19:08:32.637750 140131859355648 llama.py:54] loading llm model meta-llama/Llama-2-7b-hf
I0107 19:08:35.358273 140131859355648 llama.py:62] use device cuda:0
I0107 19:08:35.358562 140131859355648 llama.py:64] pruning starts
I0107 19:09:30.148726 140131859355648 prune.py:202] pruning layer 0
