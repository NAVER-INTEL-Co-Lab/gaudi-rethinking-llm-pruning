I0108 21:40:12.956668 140314650347520 llama.py:44] M: 0
N: 0
accumulation_steps: 1
adam_beta1: 0.9
adam_beta2: 0.95
adam_eps: 1.0e-08
batch_size: 8
cache_dir: null
epochs: 5
eval_zero_shot: false
infer_batch_size: 1
learning_rate: 0.0002
lr_scheduler: linear
max_grad_norm: 1000.0
model: meta-llama/Llama-2-7b-hf
nsamples: 256
prune_method: magnitude
seed: 0
self_nsamples: 0
seqlen: 1024
sparsity_ratio: 0.5
sparsity_type: unstructured
use_cr: false
use_fp32: true
use_gp: false
warmup_steps: 0
weight_decay: 0.0

I0108 21:40:12.959152 140314650347520 llama.py:54] loading llm model meta-llama/Llama-2-7b-hf
I0108 21:40:14.498183 140314650347520 llama.py:62] use device cuda:0
I0108 21:40:14.498344 140314650347520 llama.py:64] pruning starts
I0108 21:40:44.284407 140314650347520 prune.py:202] pruning layer 0
I0108 21:40:58.404954 140314650347520 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0108 21:41:59.513687 140314650347520 finetune.py:122] [2.1100975573062897e-05, 7.18235969543457e-06, 4.609115421772003e-06, 4.246830940246582e-06, 4.095025360584259e-06, 4.030764102935791e-06]
I0108 21:41:59.514146 140314650347520 prune.py:291] reconstruction time 68.05489730834961
I0108 21:42:04.295538 140314650347520 prune.py:295] recon error 4.019006155431271e-06
I0108 21:42:07.791095 140314650347520 prune.py:202] pruning layer 1
I0108 21:42:18.822691 140314650347520 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0108 21:43:18.869720 140314650347520 finetune.py:122] [0.0029785633087158203, 0.012076377868652344, 0.01033639907836914, 0.005397319793701172, 0.002280712127685547, 0.0008538961410522461]
I0108 21:43:18.870173 140314650347520 prune.py:291] reconstruction time 63.20657467842102
I0108 21:43:23.159392 140314650347520 prune.py:295] recon error 0.002930000424385071
I0108 21:43:26.801053 140314650347520 prune.py:202] pruning layer 2
I0108 21:43:37.684601 140314650347520 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0108 21:44:33.452086 140314650347520 finetune.py:122] [0.0002707242965698242, 0.00018455088138580322, 0.00046025216579437256, 0.00030916929244995117, 0.00022868812084197998, 0.00014613568782806396]
I0108 21:44:33.452556 140314650347520 prune.py:291] reconstruction time 58.625526905059814
I0108 21:44:38.084845 140314650347520 prune.py:295] recon error 0.003251202404499054
I0108 21:44:41.498112 140314650347520 prune.py:202] pruning layer 3
I0108 21:44:53.531113 140314650347520 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0108 21:46:00.579317 140314650347520 finetune.py:122] [0.0003876909613609314, 0.00023174285888671875, 0.0003349781036376953, 0.00036397576332092285, 0.00023946166038513184, 0.00019747018814086914]
I0108 21:46:00.579878 140314650347520 prune.py:291] reconstruction time 70.25198698043823
I0108 21:46:05.749344 140314650347520 prune.py:295] recon error 0.0035797953605651855
I0108 21:46:08.997853 140314650347520 prune.py:202] pruning layer 4
I0108 21:46:22.571185 140314650347520 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
