I0107 18:50:48.855540 140324924057600 llama.py:44] M: 0
N: 0
accumulation_steps: 1
adam_beta1: 0.9
adam_beta2: 0.95
adam_eps: 1.0e-08
batch_size: 8
cache_dir: null
epochs: 5
eval_zero_shot: false
infer_batch_size: 1
learning_rate: 0.0002
lr_scheduler: linear
max_grad_norm: 1000.0
model: meta-llama/Llama-2-7b-hf
nsamples: 256
prune_method: sparsegpt
seed: 0
self_nsamples: 0
seqlen: 1024
sparsity_ratio: 0.5
sparsity_type: unstructured
use_cr: false
use_fp32: true
use_gp: true
warmup_steps: 0
weight_decay: 0.0

I0107 18:50:48.858403 140324924057600 llama.py:54] loading llm model meta-llama/Llama-2-7b-hf
I0107 18:50:50.503078 140324924057600 llama.py:62] use device cuda:0
I0107 18:50:50.503242 140324924057600 llama.py:64] pruning starts
I0107 18:51:22.098411 140324924057600 prune.py:202] pruning layer 0
I0107 18:53:57.576947 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 18:55:06.036438 140324924057600 finetune.py:122] [1.96820474229753e-06, 2.376968041062355e-06, 1.466367393732071e-06, 1.3792887330055237e-06, 1.359032467007637e-06, 1.348322257399559e-06]
I0107 18:55:09.332004 140324924057600 prune.py:292] recon error 1.3452372513711452e-06
I0107 18:55:13.304097 140324924057600 prune.py:202] pruning layer 1
I0107 18:57:44.919656 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 18:58:44.925668 140324924057600 finetune.py:122] [0.00032242294400930405, 0.011851996183395386, 0.0067768096923828125, 0.00412440299987793, 0.0020829439163208008, 0.0006865561008453369]
I0107 18:58:47.889157 140324924057600 prune.py:292] recon error 0.0004685455933213234
I0107 18:58:51.450731 140324924057600 prune.py:202] pruning layer 2
I0107 19:01:19.621819 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:02:23.388574 140324924057600 finetune.py:122] [0.0006032362580299377, 0.0005606710910797119, 0.0005485117435455322, 0.0005347728729248047, 0.0005291402339935303, 0.0005266070365905762]
I0107 19:02:26.716068 140324924057600 prune.py:292] recon error 0.0005261506885290146
I0107 19:02:30.688168 140324924057600 prune.py:202] pruning layer 3
I0107 19:05:01.296953 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:06:00.070421 140324924057600 finetune.py:122] [0.0008059442043304443, 0.0007333159446716309, 0.0007161498069763184, 0.0006996393203735352, 0.0006872415542602539, 0.0006806254386901855]
I0107 19:06:03.291287 140324924057600 prune.py:292] recon error 0.00067877396941185
I0107 19:06:07.559972 140324924057600 prune.py:202] pruning layer 4
I0107 19:08:31.830132 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:09:34.101981 140324924057600 finetune.py:122] [0.0011281371116638184, 0.001055598258972168, 0.0010182857513427734, 0.0009816884994506836, 0.0009632110595703125, 0.0009512901306152344]
I0107 19:09:37.673728 140324924057600 prune.py:292] recon error 0.0009483247995376587
I0107 19:09:41.232876 140324924057600 prune.py:202] pruning layer 5
I0107 19:12:15.389994 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:13:12.402218 140324924057600 finetune.py:122] [0.0015783607959747314, 0.0014967918395996094, 0.0014629364013671875, 0.0013990402221679688, 0.0013570785522460938, 0.001336812973022461]
I0107 19:13:15.511590 140324924057600 prune.py:292] recon error 0.0013323575258255005
I0107 19:13:19.621825 140324924057600 prune.py:202] pruning layer 6
I0107 19:15:48.456891 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:16:48.765144 140324924057600 finetune.py:122] [0.0021479427814483643, 0.0020694732666015625, 0.002081632614135742, 0.001958608627319336, 0.0018782615661621094, 0.0018384456634521484]
I0107 19:16:51.905163 140324924057600 prune.py:292] recon error 0.0018267929553985596
I0107 19:16:55.129180 140324924057600 prune.py:202] pruning layer 7
I0107 19:19:20.661410 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:20:18.518204 140324924057600 finetune.py:122] [0.0028378963470458984, 0.002762317657470703, 0.0027074813842773438, 0.0025663375854492188, 0.002460479736328125, 0.0024166107177734375]
I0107 19:20:21.681475 140324924057600 prune.py:292] recon error 0.002405017614364624
I0107 19:20:25.124233 140324924057600 prune.py:202] pruning layer 8
I0107 19:22:56.085900 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:23:54.757237 140324924057600 finetune.py:122] [0.003660857677459717, 0.0035796165466308594, 0.0036115646362304688, 0.003360748291015625, 0.0032362937927246094, 0.003143787384033203]
I0107 19:23:57.833135 140324924057600 prune.py:292] recon error 0.003126859664916992
I0107 19:24:01.257993 140324924057600 prune.py:202] pruning layer 9
I0107 19:26:26.235756 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:27:22.783952 140324924057600 finetune.py:122] [0.004703402519226074, 0.004543304443359375, 0.004538536071777344, 0.004238128662109375, 0.004050254821777344, 0.003967761993408203]
I0107 19:27:25.893522 140324924057600 prune.py:292] recon error 0.003945410251617432
I0107 19:27:29.497108 140324924057600 prune.py:202] pruning layer 10
I0107 19:30:02.581628 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:31:02.416217 140324924057600 finetune.py:122] [0.005772113800048828, 0.005631446838378906, 0.0054874420166015625, 0.005187034606933594, 0.004992485046386719, 0.0048885345458984375]
I0107 19:31:05.739186 140324924057600 prune.py:292] recon error 0.00486379861831665
I0107 19:31:09.069581 140324924057600 prune.py:202] pruning layer 11
I0107 19:33:26.996410 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:34:24.216588 140324924057600 finetune.py:122] [0.0066661834716796875, 0.0065898895263671875, 0.006445884704589844, 0.006066322326660156, 0.005820274353027344, 0.005705833435058594]
I0107 19:34:27.378401 140324924057600 prune.py:292] recon error 0.005673408508300781
I0107 19:34:30.688825 140324924057600 prune.py:202] pruning layer 12
I0107 19:36:51.891528 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:37:48.120937 140324924057600 finetune.py:122] [0.007817625999450684, 0.007693290710449219, 0.0074481964111328125, 0.007039070129394531, 0.006793022155761719, 0.00667572021484375]
I0107 19:37:51.070875 140324924057600 prune.py:292] recon error 0.006637692451477051
I0107 19:37:53.902670 140324924057600 prune.py:202] pruning layer 13
I0107 19:40:11.559607 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:41:08.982639 140324924057600 finetune.py:122] [0.00917351245880127, 0.00893402099609375, 0.008778572082519531, 0.008234024047851562, 0.007905960083007812, 0.0077667236328125]
I0107 19:41:12.014787 140324924057600 prune.py:292] recon error 0.007722973823547363
I0107 19:41:15.153956 140324924057600 prune.py:202] pruning layer 14
I0107 19:43:38.888296 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:44:35.688791 140324924057600 finetune.py:122] [0.010571002960205078, 0.010494232177734375, 0.01000213623046875, 0.009668350219726562, 0.009258270263671875, 0.009088516235351562]
I0107 19:44:38.711570 140324924057600 prune.py:292] recon error 0.009030461311340332
I0107 19:44:41.673179 140324924057600 prune.py:202] pruning layer 15
I0107 19:46:57.921615 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:47:54.345517 140324924057600 finetune.py:122] [0.012615203857421875, 0.012359619140625, 0.0119476318359375, 0.0113067626953125, 0.010946273803710938, 0.010751724243164062]
I0107 19:47:57.450474 140324924057600 prune.py:292] recon error 0.01069629192352295
I0107 19:48:00.563368 140324924057600 prune.py:202] pruning layer 16
I0107 19:50:21.820952 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:51:17.839997 140324924057600 finetune.py:122] [0.01616835594177246, 0.015867233276367188, 0.015451431274414062, 0.014623641967773438, 0.014024734497070312, 0.013751983642578125]
I0107 19:51:20.837968 140324924057600 prune.py:292] recon error 0.013677835464477539
I0107 19:51:23.751764 140324924057600 prune.py:202] pruning layer 17
I0107 19:53:38.788072 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:54:35.557083 140324924057600 finetune.py:122] [0.01902151107788086, 0.01876068115234375, 0.019756317138671875, 0.01795196533203125, 0.016984939575195312, 0.016628265380859375]
I0107 19:54:38.498950 140324924057600 prune.py:292] recon error 0.01654338836669922
I0107 19:54:41.544346 140324924057600 prune.py:202] pruning layer 18
I0107 19:57:01.294581 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:57:58.971934 140324924057600 finetune.py:122] [0.02388906478881836, 0.02338409423828125, 0.024097442626953125, 0.0216827392578125, 0.02098846435546875, 0.020664215087890625]
I0107 19:58:01.902483 140324924057600 prune.py:292] recon error 0.020543575286865234
I0107 19:58:05.195466 140324924057600 prune.py:202] pruning layer 19
I0107 20:00:20.640240 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:01:15.090977 140324924057600 finetune.py:122] [0.02894115447998047, 0.028507232666015625, 0.0277557373046875, 0.026409149169921875, 0.025730133056640625, 0.0254058837890625]
I0107 20:01:18.127459 140324924057600 prune.py:292] recon error 0.025299549102783203
I0107 20:01:21.227965 140324924057600 prune.py:202] pruning layer 20
I0107 20:03:41.473664 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:04:35.704721 140324924057600 finetune.py:122] [0.03608369827270508, 0.035614013671875, 0.0355682373046875, 0.033588409423828125, 0.032459259033203125, 0.031887054443359375]
I0107 20:04:38.588226 140324924057600 prune.py:292] recon error 0.031764984130859375
I0107 20:04:41.457076 140324924057600 prune.py:202] pruning layer 21
I0107 20:06:55.947217 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:07:54.009133 140324924057600 finetune.py:122] [0.042845726013183594, 0.04242706298828125, 0.0420989990234375, 0.0399169921875, 0.03887939453125, 0.03838348388671875]
I0107 20:07:57.043481 140324924057600 prune.py:292] recon error 0.0382542610168457
I0107 20:08:00.133350 140324924057600 prune.py:202] pruning layer 22
I0107 20:10:19.656220 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:11:15.852662 140324924057600 finetune.py:122] [0.05232048034667969, 0.05171966552734375, 0.05098724365234375, 0.04865264892578125, 0.0476837158203125, 0.0469970703125]
I0107 20:11:18.882690 140324924057600 prune.py:292] recon error 0.04680967330932617
I0107 20:11:21.900141 140324924057600 prune.py:202] pruning layer 23
I0107 20:13:40.066806 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:14:32.708140 140324924057600 finetune.py:122] [0.062172889709472656, 0.06114959716796875, 0.06171417236328125, 0.05792236328125, 0.05673980712890625, 0.05600738525390625]
I0107 20:14:35.639782 140324924057600 prune.py:292] recon error 0.055794715881347656
I0107 20:14:39.245689 140324924057600 prune.py:202] pruning layer 24
I0107 20:17:11.091347 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:18:05.533633 140324924057600 finetune.py:122] [0.07305240631103516, 0.0720977783203125, 0.071197509765625, 0.0681304931640625, 0.06668853759765625, 0.066070556640625]
I0107 20:18:08.440095 140324924057600 prune.py:292] recon error 0.0658712387084961
I0107 20:18:11.400570 140324924057600 prune.py:202] pruning layer 25
I0107 20:20:29.487533 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:21:25.527367 140324924057600 finetune.py:122] [0.08378887176513672, 0.0830841064453125, 0.081756591796875, 0.0791168212890625, 0.0775909423828125, 0.0767822265625]
I0107 20:21:28.415747 140324924057600 prune.py:292] recon error 0.07660293579101562
I0107 20:21:31.492430 140324924057600 prune.py:202] pruning layer 26
I0107 20:23:54.990345 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:24:52.792711 140324924057600 finetune.py:122] [0.09961414337158203, 0.0983734130859375, 0.096832275390625, 0.093475341796875, 0.091888427734375, 0.0908203125]
I0107 20:24:55.697380 140324924057600 prune.py:292] recon error 0.09058761596679688
I0107 20:24:58.615034 140324924057600 prune.py:202] pruning layer 27
I0107 20:27:15.861541 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:28:12.934399 140324924057600 finetune.py:122] [0.11609268188476562, 0.114654541015625, 0.1127166748046875, 0.1090087890625, 0.1071014404296875, 0.106201171875]
I0107 20:28:16.466466 140324924057600 prune.py:292] recon error 0.10584640502929688
I0107 20:28:19.805286 140324924057600 prune.py:202] pruning layer 28
I0107 20:30:45.704434 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:31:43.933722 140324924057600 finetune.py:122] [0.13703536987304688, 0.134918212890625, 0.1356201171875, 0.1291656494140625, 0.126556396484375, 0.125244140625]
I0107 20:31:46.921976 140324924057600 prune.py:292] recon error 0.12482833862304688
I0107 20:31:50.001819 140324924057600 prune.py:202] pruning layer 29
I0107 20:34:07.394635 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:35:04.920771 140324924057600 finetune.py:122] [0.1611461639404297, 0.158477783203125, 0.1611328125, 0.15185546875, 0.148773193359375, 0.147247314453125]
I0107 20:35:07.921412 140324924057600 prune.py:292] recon error 0.1467761993408203
I0107 20:35:11.426261 140324924057600 prune.py:202] pruning layer 30
I0107 20:37:32.965445 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:38:28.033855 140324924057600 finetune.py:122] [0.20050430297851562, 0.19439697265625, 0.303314208984375, 0.18621826171875, 0.18194580078125, 0.1795654296875]
I0107 20:38:30.956151 140324924057600 prune.py:292] recon error 0.17886924743652344
I0107 20:38:33.962172 140324924057600 prune.py:202] pruning layer 31
I0107 20:40:52.618411 140324924057600 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:41:49.767076 140324924057600 finetune.py:122] [0.3159141540527344, 0.32257080078125, 0.3482666015625, 0.30255126953125, 0.290771484375, 0.2838134765625]
I0107 20:41:53.036686 140324924057600 prune.py:292] recon error 0.2818412780761719
I0107 20:41:56.190911 140324924057600 llama.py:71] ******************************
I0107 20:41:56.285120 140324924057600 prune.py:59] block 0 sparsity 0.500002
I0107 20:41:56.374196 140324924057600 prune.py:59] block 1 sparsity 0.500002
I0107 20:41:56.470918 140324924057600 prune.py:59] block 2 sparsity 0.500002
I0107 20:41:56.565384 140324924057600 prune.py:59] block 3 sparsity 0.500002
I0107 20:41:56.665648 140324924057600 prune.py:59] block 4 sparsity 0.500002
I0107 20:41:56.788782 140324924057600 prune.py:59] block 5 sparsity 0.500002
I0107 20:41:56.887263 140324924057600 prune.py:59] block 6 sparsity 0.500002
I0107 20:41:56.986244 140324924057600 prune.py:59] block 7 sparsity 0.500002
I0107 20:41:57.080659 140324924057600 prune.py:59] block 8 sparsity 0.500001
I0107 20:41:57.176194 140324924057600 prune.py:59] block 9 sparsity 0.500002
I0107 20:41:57.277149 140324924057600 prune.py:59] block 10 sparsity 0.500001
I0107 20:41:57.371255 140324924057600 prune.py:59] block 11 sparsity 0.500002
I0107 20:41:57.468942 140324924057600 prune.py:59] block 12 sparsity 0.500002
I0107 20:41:57.564612 140324924057600 prune.py:59] block 13 sparsity 0.500002
I0107 20:41:57.669302 140324924057600 prune.py:59] block 14 sparsity 0.500002
I0107 20:41:57.771150 140324924057600 prune.py:59] block 15 sparsity 0.500002
I0107 20:41:57.865730 140324924057600 prune.py:59] block 16 sparsity 0.500002
I0107 20:41:57.961850 140324924057600 prune.py:59] block 17 sparsity 0.500001
I0107 20:41:58.057767 140324924057600 prune.py:59] block 18 sparsity 0.500002
I0107 20:41:58.151052 140324924057600 prune.py:59] block 19 sparsity 0.500002
I0107 20:41:58.251339 140324924057600 prune.py:59] block 20 sparsity 0.500002
I0107 20:41:58.344985 140324924057600 prune.py:59] block 21 sparsity 0.500002
I0107 20:41:58.447316 140324924057600 prune.py:59] block 22 sparsity 0.500002
I0107 20:41:58.543584 140324924057600 prune.py:59] block 23 sparsity 0.500002
I0107 20:41:58.663403 140324924057600 prune.py:59] block 24 sparsity 0.500002
I0107 20:41:58.755637 140324924057600 prune.py:59] block 25 sparsity 0.500001
I0107 20:41:58.854439 140324924057600 prune.py:59] block 26 sparsity 0.500001
I0107 20:41:58.949537 140324924057600 prune.py:59] block 27 sparsity 0.500002
I0107 20:41:59.045895 140324924057600 prune.py:59] block 28 sparsity 0.500002
I0107 20:41:59.142574 140324924057600 prune.py:59] block 29 sparsity 0.500002
I0107 20:41:59.211673 140324924057600 prune.py:59] block 30 sparsity 0.500002
I0107 20:41:59.281329 140324924057600 prune.py:59] block 31 sparsity 0.500002
I0107 20:41:59.281537 140324924057600 llama.py:73] sparsity sanity check 0.5000
I0107 20:41:59.281581 140324924057600 llama.py:74] ******************************
I0107 20:42:05.186182 140324924057600 eval.py:26] evaluating on wikitext2
I0107 20:43:04.542068 140324924057600 eval.py:48] nsamples 83
I0107 20:43:04.542268 140324924057600 eval.py:53] sample 0
I0107 20:43:43.826367 140324924057600 eval.py:53] sample 50
I0107 20:44:09.747925 140324924057600 eval.py:18] wikitext2 perplexity 6.203049182891846
I0107 20:44:09.748090 140324924057600 eval.py:26] evaluating on ptb
I0107 20:44:23.149375 140324924057600 eval.py:48] nsamples 24
I0107 20:44:23.149541 140324924057600 eval.py:53] sample 0
I0107 20:44:42.000648 140324924057600 eval.py:18] ptb perplexity 7683.70751953125
I0107 20:44:42.000809 140324924057600 eval.py:26] evaluating on c4
I0107 20:45:42.597583 140324924057600 eval.py:48] nsamples 149
I0107 20:45:42.597760 140324924057600 eval.py:53] sample 0
I0107 20:46:21.870830 140324924057600 eval.py:53] sample 50
I0107 20:47:01.134675 140324924057600 eval.py:53] sample 100
I0107 20:47:39.612728 140324924057600 eval.py:18] c4 perplexity 8.3016939163208
