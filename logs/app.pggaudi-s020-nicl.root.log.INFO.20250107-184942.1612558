I0107 18:49:42.573213 140285751068672 llama.py:44] M: 0
N: 0
accumulation_steps: 1
adam_beta1: 0.9
adam_beta2: 0.95
adam_eps: 1.0e-08
batch_size: 8
cache_dir: null
epochs: 5
eval_zero_shot: false
infer_batch_size: 1
learning_rate: 0.0002
lr_scheduler: linear
max_grad_norm: 1000.0
model: meta-llama/Llama-2-7b-hf
nsamples: 256
prune_method: sparsegpt
seed: 0
self_nsamples: 0
seqlen: 1024
sparsity_ratio: 0.5
sparsity_type: unstructured
use_cr: false
use_fp32: true
use_gp: false
warmup_steps: 0
weight_decay: 0.0

I0107 18:49:42.575677 140285751068672 llama.py:54] loading llm model meta-llama/Llama-2-7b-hf
I0107 18:49:44.126127 140285751068672 llama.py:62] use device cuda:0
I0107 18:49:44.126305 140285751068672 llama.py:64] pruning starts
I0107 18:50:15.753130 140285751068672 prune.py:202] pruning layer 0
I0107 18:52:45.066316 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 18:53:48.187306 140285751068672 finetune.py:122] [1.96820474229753e-06, 2.376968041062355e-06, 1.466367393732071e-06, 1.3792887330055237e-06, 1.359032467007637e-06, 1.348322257399559e-06]
I0107 18:53:52.980767 140285751068672 prune.py:292] recon error 1.3452372513711452e-06
I0107 18:53:56.843469 140285751068672 prune.py:202] pruning layer 1
I0107 18:56:25.903119 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 18:57:24.082311 140285751068672 finetune.py:122] [0.0001439000479876995, 0.011615768074989319, 0.008374214172363281, 0.005085408687591553, 0.0017633438110351562, 0.000641167163848877]
I0107 18:57:27.953064 140285751068672 prune.py:292] recon error 0.0005538621917366982
I0107 18:57:31.058493 140285751068672 prune.py:202] pruning layer 2
I0107 18:59:52.784909 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:00:51.199775 140285751068672 finetune.py:122] [8.144043385982513e-05, 6.292760372161865e-05, 6.189197301864624e-05, 5.5812299251556396e-05, 5.40614128112793e-05, 5.3219497203826904e-05]
I0107 19:00:55.628292 140285751068672 prune.py:292] recon error 0.0006514731794595718
I0107 19:00:59.087038 140285751068672 prune.py:202] pruning layer 3
I0107 19:03:22.757353 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:04:21.976976 140285751068672 finetune.py:122] [0.00020741671323776245, 0.00014728307723999023, 0.0001407414674758911, 0.00012764334678649902, 0.00012083351612091064, 0.00011713802814483643]
I0107 19:04:26.048914 140285751068672 prune.py:292] recon error 0.0008410289883613586
I0107 19:04:29.649243 140285751068672 prune.py:202] pruning layer 4
I0107 19:06:49.735208 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:07:49.663517 140285751068672 finetune.py:122] [0.000339508056640625, 0.00027379393577575684, 0.0002645552158355713, 0.0002428591251373291, 0.00022354722023010254, 0.00021457672119140625]
I0107 19:07:53.822298 140285751068672 prune.py:292] recon error 0.0011834055185317993
I0107 19:07:57.274226 140285751068672 prune.py:202] pruning layer 5
I0107 19:10:21.009286 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:11:19.030802 140285751068672 finetune.py:122] [0.0005194768309593201, 0.00043976306915283203, 0.0004324913024902344, 0.00043708086013793945, 0.00035858154296875, 0.0003433823585510254]
I0107 19:11:23.037431 140285751068672 prune.py:292] recon error 0.0016787797212600708
I0107 19:11:26.168535 140285751068672 prune.py:202] pruning layer 6
I0107 19:13:49.698413 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:14:49.987102 140285751068672 finetune.py:122] [0.0006997436285018921, 0.0006288290023803711, 0.0006835460662841797, 0.0005941390991210938, 0.0005230903625488281, 0.0004914999008178711]
I0107 19:14:54.142729 140285751068672 prune.py:292] recon error 0.0023431777954101562
I0107 19:14:57.396209 140285751068672 prune.py:202] pruning layer 7
I0107 19:17:29.623030 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:18:26.796546 140285751068672 finetune.py:122] [0.0009239614009857178, 0.0008480548858642578, 0.0008999109268188477, 0.0007536411285400391, 0.000675201416015625, 0.0006428956985473633]
I0107 19:18:30.844972 140285751068672 prune.py:292] recon error 0.003173530101776123
I0107 19:18:34.928815 140285751068672 prune.py:202] pruning layer 8
I0107 19:21:02.031869 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:22:01.462435 140285751068672 finetune.py:122] [0.0011782795190811157, 0.0010938644409179688, 0.0012143850326538086, 0.0009715557098388672, 0.0009199380874633789, 0.0008227825164794922]
I0107 19:22:05.793769 140285751068672 prune.py:292] recon error 0.004250288009643555
I0107 19:22:10.341143 140285751068672 prune.py:202] pruning layer 9
I0107 19:24:34.692217 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:25:30.871989 140285751068672 finetune.py:122] [0.001524507999420166, 0.0013532638549804688, 0.0014972686767578125, 0.0012080669403076172, 0.001070261001586914, 0.001014113426208496]
I0107 19:25:35.777397 140285751068672 prune.py:292] recon error 0.005567729473114014
I0107 19:25:39.699018 140285751068672 prune.py:202] pruning layer 10
I0107 19:28:04.093699 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:29:02.980063 140285751068672 finetune.py:122] [0.001817852258682251, 0.001672506332397461, 0.0016667842864990234, 0.0014235973358154297, 0.0012731552124023438, 0.0012018680572509766]
I0107 19:29:06.982038 140285751068672 prune.py:292] recon error 0.007043004035949707
I0107 19:29:10.156129 140285751068672 prune.py:202] pruning layer 11
I0107 19:31:35.495235 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:32:30.252304 140285751068672 finetune.py:122] [0.0019834041595458984, 0.0019378662109375, 0.0019154548645019531, 0.001631021499633789, 0.0014264583587646484, 0.0013425350189208984]
I0107 19:32:33.983580 140285751068672 prune.py:292] recon error 0.008457303047180176
I0107 19:32:37.133167 140285751068672 prune.py:202] pruning layer 12
I0107 19:34:58.068850 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:35:54.265901 140285751068672 finetune.py:122] [0.0023413896560668945, 0.0021729469299316406, 0.002253293991088867, 0.0018684864044189453, 0.0017676353454589844, 0.0015668869018554688]
I0107 19:35:57.814788 140285751068672 prune.py:292] recon error 0.010197997093200684
I0107 19:36:01.734739 140285751068672 prune.py:202] pruning layer 13
I0107 19:38:23.172779 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:39:18.096562 140285751068672 finetune.py:122] [0.0028628408908843994, 0.002533435821533203, 0.002552032470703125, 0.0022449493408203125, 0.0019490718841552734, 0.0018334388732910156]
I0107 19:39:21.805864 140285751068672 prune.py:292] recon error 0.012171268463134766
I0107 19:39:24.759899 140285751068672 prune.py:202] pruning layer 14
I0107 19:41:44.853096 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:42:43.509346 140285751068672 finetune.py:122] [0.0030933916568756104, 0.0030226707458496094, 0.0036797523498535156, 0.0025496482849121094, 0.002269268035888672, 0.0021495819091796875]
I0107 19:42:47.536390 140285751068672 prune.py:292] recon error 0.014552593231201172
I0107 19:42:51.734040 140285751068672 prune.py:202] pruning layer 15
I0107 19:45:11.844514 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:46:08.616786 140285751068672 finetune.py:122] [0.003819406032562256, 0.00354766845703125, 0.0033588409423828125, 0.0029783248901367188, 0.002689361572265625, 0.0025625228881835938]
I0107 19:46:12.460874 140285751068672 prune.py:292] recon error 0.01758861541748047
I0107 19:46:15.807703 140285751068672 prune.py:202] pruning layer 16
I0107 19:48:38.455677 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:49:34.763748 140285751068672 finetune.py:122] [0.005061984062194824, 0.0047206878662109375, 0.0047359466552734375, 0.004155158996582031, 0.003647327423095703, 0.0034394264221191406]
I0107 19:49:38.627961 140285751068672 prune.py:292] recon error 0.02301764488220215
I0107 19:49:41.473359 140285751068672 prune.py:202] pruning layer 17
I0107 19:52:01.195281 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:52:56.377523 140285751068672 finetune.py:122] [0.0053136348724365234, 0.004975318908691406, 0.0066432952880859375, 0.004902839660644531, 0.003993034362792969, 0.0037398338317871094]
I0107 19:53:00.174660 140285751068672 prune.py:292] recon error 0.02807760238647461
I0107 19:53:03.053032 140285751068672 prune.py:202] pruning layer 18
I0107 19:55:20.475202 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:56:16.781460 140285751068672 finetune.py:122] [0.006284058094024658, 0.00576019287109375, 0.006084442138671875, 0.0050601959228515625, 0.004506111145019531, 0.004296302795410156]
I0107 19:56:20.679181 140285751068672 prune.py:292] recon error 0.03574848175048828
I0107 19:56:23.574545 140285751068672 prune.py:202] pruning layer 19
I0107 19:58:40.546896 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 19:59:34.798075 140285751068672 finetune.py:122] [0.006849765777587891, 0.006336212158203125, 0.0062046051025390625, 0.005451202392578125, 0.004932403564453125, 0.004710197448730469]
I0107 19:59:38.116806 140285751068672 prune.py:292] recon error 0.04498577117919922
I0107 19:59:41.034687 140285751068672 prune.py:202] pruning layer 20
I0107 20:01:58.037864 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:02:55.605111 140285751068672 finetune.py:122] [0.008223891258239746, 0.007676124572753906, 0.008521080017089844, 0.006896018981933594, 0.0061016082763671875, 0.005705833435058594]
I0107 20:02:58.955705 140285751068672 prune.py:292] recon error 0.058139801025390625
I0107 20:03:01.909891 140285751068672 prune.py:202] pruning layer 21
I0107 20:05:21.405983 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:06:16.019695 140285751068672 finetune.py:122] [0.008158564567565918, 0.007724761962890625, 0.008496284484863281, 0.006903648376464844, 0.006072044372558594, 0.0057697296142578125]
I0107 20:06:19.805004 140285751068672 prune.py:292] recon error 0.07136249542236328
I0107 20:06:22.712422 140285751068672 prune.py:202] pruning layer 22
I0107 20:08:39.026845 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:09:35.487849 140285751068672 finetune.py:122] [0.009988784790039062, 0.009016036987304688, 0.009675979614257812, 0.008074760437011719, 0.0072078704833984375, 0.0067729949951171875]
I0107 20:09:39.127265 140285751068672 prune.py:292] recon error 0.08966255187988281
I0107 20:09:42.063785 140285751068672 prune.py:202] pruning layer 23
I0107 20:12:03.347980 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:12:58.171552 140285751068672 finetune.py:122] [0.010689616203308105, 0.009725570678710938, 0.010814666748046875, 0.008564949035644531, 0.007775306701660156, 0.007380485534667969]
I0107 20:13:01.881884 140285751068672 prune.py:292] recon error 0.10767555236816406
I0107 20:13:04.622208 140285751068672 prune.py:202] pruning layer 24
I0107 20:15:22.646370 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:16:17.820378 140285751068672 finetune.py:122] [0.011655688285827637, 0.010591506958007812, 0.011770248413085938, 0.009237289428710938, 0.008275032043457031, 0.007912635803222656]
I0107 20:16:21.990502 140285751068672 prune.py:292] recon error 0.12961196899414062
I0107 20:16:25.734704 140285751068672 prune.py:202] pruning layer 25
I0107 20:18:45.878504 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:19:40.252563 140285751068672 finetune.py:122] [0.01149904727935791, 0.010892868041992188, 0.01189422607421875, 0.009685516357421875, 0.008775711059570312, 0.008287429809570312]
I0107 20:19:43.951384 140285751068672 prune.py:292] recon error 0.15133094787597656
I0107 20:19:47.409152 140285751068672 prune.py:202] pruning layer 26
I0107 20:22:05.407468 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:23:02.332758 140285751068672 finetune.py:122] [0.014541029930114746, 0.013172149658203125, 0.014284133911132812, 0.011533737182617188, 0.010410308837890625, 0.00981903076171875]
I0107 20:23:06.164499 140285751068672 prune.py:292] recon error 0.1815338134765625
I0107 20:23:09.271507 140285751068672 prune.py:202] pruning layer 27
I0107 20:25:30.883730 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:26:25.240040 140285751068672 finetune.py:122] [0.015848398208618164, 0.01415252685546875, 0.01573944091796875, 0.012315750122070312, 0.011205673217773438, 0.0106658935546875]
I0107 20:26:28.931324 140285751068672 prune.py:292] recon error 0.21310806274414062
I0107 20:26:31.676161 140285751068672 prune.py:202] pruning layer 28
I0107 20:28:50.868981 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:29:46.910437 140285751068672 finetune.py:122] [0.019103527069091797, 0.016740798950195312, 0.021329879760742188, 0.015041351318359375, 0.013507843017578125, 0.012620925903320312]
I0107 20:29:50.002828 140285751068672 prune.py:292] recon error 0.2536163330078125
I0107 20:29:52.835727 140285751068672 prune.py:202] pruning layer 29
I0107 20:32:12.472250 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:33:09.266738 140285751068672 finetune.py:122] [0.021334171295166016, 0.01857757568359375, 0.024354934692382812, 0.016855239868164062, 0.014881134033203125, 0.01404571533203125]
I0107 20:33:12.843102 140285751068672 prune.py:292] recon error 0.30113983154296875
I0107 20:33:15.624725 140285751068672 prune.py:202] pruning layer 30
I0107 20:35:34.793494 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:36:32.702001 140285751068672 finetune.py:122] [0.03282356262207031, 0.02508544921875, 0.026477813720703125, 0.0214691162109375, 0.0193328857421875, 0.017963409423828125]
I0107 20:36:35.700434 140285751068672 prune.py:292] recon error 0.3730506896972656
I0107 20:36:38.508074 140285751068672 prune.py:202] pruning layer 31
I0107 20:38:57.609360 140285751068672 finetune.py:128] weight params, number of params: 202383360, weight_decay: 0.0, lr: 0.0002
I0107 20:39:53.718427 140285751068672 finetune.py:122] [0.05142545700073242, 0.0579986572265625, 0.083953857421875, 0.05040740966796875, 0.0388641357421875, 0.0342254638671875]
I0107 20:39:57.223106 140285751068672 prune.py:292] recon error 0.6439208984375
I0107 20:40:00.114254 140285751068672 llama.py:71] ******************************
I0107 20:40:00.213700 140285751068672 prune.py:59] block 0 sparsity 0.500002
I0107 20:40:00.307991 140285751068672 prune.py:59] block 1 sparsity 0.500002
I0107 20:40:00.397346 140285751068672 prune.py:59] block 2 sparsity 0.500002
I0107 20:40:00.493263 140285751068672 prune.py:59] block 3 sparsity 0.500002
I0107 20:40:00.579982 140285751068672 prune.py:59] block 4 sparsity 0.500002
I0107 20:40:00.670052 140285751068672 prune.py:59] block 5 sparsity 0.500002
I0107 20:40:00.760931 140285751068672 prune.py:59] block 6 sparsity 0.500002
I0107 20:40:00.853838 140285751068672 prune.py:59] block 7 sparsity 0.500002
I0107 20:40:00.944042 140285751068672 prune.py:59] block 8 sparsity 0.500002
I0107 20:40:01.033435 140285751068672 prune.py:59] block 9 sparsity 0.500002
I0107 20:40:01.126020 140285751068672 prune.py:59] block 10 sparsity 0.500002
I0107 20:40:01.214093 140285751068672 prune.py:59] block 11 sparsity 0.500002
I0107 20:40:01.304540 140285751068672 prune.py:59] block 12 sparsity 0.500002
I0107 20:40:01.396264 140285751068672 prune.py:59] block 13 sparsity 0.500002
I0107 20:40:01.489719 140285751068672 prune.py:59] block 14 sparsity 0.500002
I0107 20:40:01.581517 140285751068672 prune.py:59] block 15 sparsity 0.500002
I0107 20:40:01.674214 140285751068672 prune.py:59] block 16 sparsity 0.500002
I0107 20:40:01.765104 140285751068672 prune.py:59] block 17 sparsity 0.500002
I0107 20:40:01.858136 140285751068672 prune.py:59] block 18 sparsity 0.500002
I0107 20:40:01.948562 140285751068672 prune.py:59] block 19 sparsity 0.500002
I0107 20:40:02.037481 140285751068672 prune.py:59] block 20 sparsity 0.500002
I0107 20:40:02.129822 140285751068672 prune.py:59] block 21 sparsity 0.500002
I0107 20:40:02.220086 140285751068672 prune.py:59] block 22 sparsity 0.500002
I0107 20:40:02.312435 140285751068672 prune.py:59] block 23 sparsity 0.500002
I0107 20:40:02.404063 140285751068672 prune.py:59] block 24 sparsity 0.500002
I0107 20:40:02.497571 140285751068672 prune.py:59] block 25 sparsity 0.500001
I0107 20:40:02.588722 140285751068672 prune.py:59] block 26 sparsity 0.500002
I0107 20:40:02.683946 140285751068672 prune.py:59] block 27 sparsity 0.500002
I0107 20:40:02.781709 140285751068672 prune.py:59] block 28 sparsity 0.500002
I0107 20:40:02.878885 140285751068672 prune.py:59] block 29 sparsity 0.500002
I0107 20:40:02.970767 140285751068672 prune.py:59] block 30 sparsity 0.500002
I0107 20:40:03.036844 140285751068672 prune.py:59] block 31 sparsity 0.500002
I0107 20:40:03.037036 140285751068672 llama.py:73] sparsity sanity check 0.5000
I0107 20:40:03.037078 140285751068672 llama.py:74] ******************************
I0107 20:40:07.689620 140285751068672 eval.py:26] evaluating on wikitext2
I0107 20:41:04.155962 140285751068672 eval.py:48] nsamples 83
I0107 20:41:04.156127 140285751068672 eval.py:53] sample 0
I0107 20:41:43.440938 140285751068672 eval.py:53] sample 50
I0107 20:42:09.394562 140285751068672 eval.py:18] wikitext2 perplexity 6.329071521759033
I0107 20:42:09.394739 140285751068672 eval.py:26] evaluating on ptb
I0107 20:42:23.505536 140285751068672 eval.py:48] nsamples 24
I0107 20:42:23.505701 140285751068672 eval.py:53] sample 0
I0107 20:42:42.371792 140285751068672 eval.py:18] ptb perplexity 6316.07666015625
I0107 20:42:42.371959 140285751068672 eval.py:26] evaluating on c4
I0107 20:43:43.546640 140285751068672 eval.py:48] nsamples 149
I0107 20:43:43.546822 140285751068672 eval.py:53] sample 0
I0107 20:44:22.815559 140285751068672 eval.py:53] sample 50
I0107 20:45:02.090280 140285751068672 eval.py:53] sample 100
I0107 20:45:40.572391 140285751068672 eval.py:18] c4 perplexity 8.4051513671875
