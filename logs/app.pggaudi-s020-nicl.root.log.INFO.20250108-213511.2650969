I0108 21:35:11.120030 139932690765824 opt.py:30] M: 0
N: 0
accumulation_steps: 1
adam_beta1: 0.9
adam_beta2: 0.95
adam_eps: 1.0e-08
batch_size: 8
cache_dir: llm_weights
epochs: 10
eval_zero_shot: false
infer_batch_size: 1
learning_rate: 0.0002
lr_scheduler: linear
max_grad_norm: 1000.0
model: facebook/opt-125m
nsamples: 256
prune_method: magnitude
seed: 0
self_nsamples: 0
seqlen: 1024
sparsity_ratio: 0.5
sparsity_type: unstructured
use_cr: false
use_fp32: true
use_gp: false
warmup_steps: 0
weight_decay: 0.0

I0108 21:35:11.122636 139932690765824 opt.py:40] loading llm model facebook/opt-125m
I0108 21:35:12.320165 139932690765824 opt.py:48] use device cuda:0
I0108 21:35:12.320315 139932690765824 opt.py:50] pruning starts
I0108 21:35:36.854487 139932690765824 prune.py:207] pruning layer 0
I0108 21:35:38.884388 139932690765824 finetune.py:129] weight params, number of params: 7087872, weight_decay: 0.0, lr: 0.0002
I0108 21:35:48.862157 139932690765824 finetune.py:123] [0.00027037039399147034, 0.0001627206802368164, 0.0001074373722076416, 8.90195369720459e-05, 8.131563663482666e-05, 7.683038711547852e-05, 7.325410842895508e-05, 7.101893424987793e-05, 6.921589374542236e-05, 6.821751594543457e-05, 6.759166717529297e-05]
I0108 21:35:49.651314 139932690765824 prune.py:307] recon error 6.745290011167526e-05
I0108 21:35:50.414906 139932690765824 prune.py:207] pruning layer 1
I0108 21:35:52.189632 139932690765824 finetune.py:129] weight params, number of params: 7087872, weight_decay: 0.0, lr: 0.0002
I0108 21:36:02.222422 139932690765824 finetune.py:123] [0.0004646778106689453, 0.00018543004989624023, 7.463991641998291e-05, 6.010383367538452e-05, 5.3748488426208496e-05, 5.020946264266968e-05, 4.769861698150635e-05, 4.611164331436157e-05, 4.503875970840454e-05, 4.4383108615875244e-05, 4.3973326683044434e-05]
I0108 21:36:03.078310 139932690765824 prune.py:307] recon error 0.00012007169425487518
I0108 21:36:03.858254 139932690765824 prune.py:207] pruning layer 2
I0108 21:36:05.688635 139932690765824 finetune.py:129] weight params, number of params: 7087872, weight_decay: 0.0, lr: 0.0002
I0108 21:36:15.666214 139932690765824 finetune.py:123] [0.00023190677165985107, 0.0001049339771270752, 6.407499313354492e-05, 5.4933130741119385e-05, 4.967302083969116e-05, 4.706531763076782e-05, 4.5180320739746094e-05, 4.394352436065674e-05, 4.29302453994751e-05, 4.2378902435302734e-05, 4.203617572784424e-05]
I0108 21:36:16.449228 139932690765824 prune.py:307] recon error 0.00018559396266937256
I0108 21:36:17.217428 139932690765824 prune.py:207] pruning layer 3
I0108 21:36:19.022363 139932690765824 finetune.py:129] weight params, number of params: 7087872, weight_decay: 0.0, lr: 0.0002
I0108 21:36:28.865657 139932690765824 finetune.py:123] [0.0001473948359489441, 6.805360317230225e-05, 3.927946090698242e-05, 3.3624470233917236e-05, 3.267824649810791e-05, 2.86102294921875e-05, 2.742186188697815e-05, 2.650916576385498e-05, 2.6036053895950317e-05, 2.5637447834014893e-05, 2.53431499004364e-05]
I0108 21:36:29.732117 139932690765824 prune.py:307] recon error 0.00023905932903289795
I0108 21:36:30.562884 139932690765824 prune.py:207] pruning layer 4
I0108 21:36:32.360949 139932690765824 finetune.py:129] weight params, number of params: 7087872, weight_decay: 0.0, lr: 0.0002
I0108 21:36:42.007426 139932690765824 finetune.py:123] [0.000610053539276123, 0.0002582073211669922, 0.000152587890625, 0.0001293867826461792, 0.00011798739433288574, 0.00011152029037475586, 0.00010748207569122314, 0.00010468065738677979, 0.00010284781455993652, 0.00010159611701965332, 0.00010086596012115479]
I0108 21:36:42.769908 139932690765824 prune.py:307] recon error 0.0003877878189086914
I0108 21:36:43.519588 139932690765824 prune.py:207] pruning layer 5
I0108 21:36:45.299879 139932690765824 finetune.py:129] weight params, number of params: 7087872, weight_decay: 0.0, lr: 0.0002
I0108 21:36:54.812632 139932690765824 finetune.py:123] [0.0006969720125198364, 0.00036770105361938477, 0.0002506673336029053, 0.00021967291831970215, 0.00020372867584228516, 0.00019478797912597656, 0.0001887977123260498, 0.0001849830150604248, 0.000182420015335083, 0.0001806318759918213, 0.00017967820167541504]
I0108 21:36:55.583183 139932690765824 prune.py:307] recon error 0.000629916787147522
I0108 21:36:56.328552 139932690765824 prune.py:207] pruning layer 6
I0108 21:36:58.085730 139932690765824 finetune.py:129] weight params, number of params: 7087872, weight_decay: 0.0, lr: 0.0002
I0108 21:37:07.726210 139932690765824 finetune.py:123] [0.0009675770998001099, 0.0005043148994445801, 0.0003326535224914551, 0.00028818845748901367, 0.00026613473892211914, 0.00025391578674316406, 0.0002461373805999756, 0.0002409219741821289, 0.00023737549781799316, 0.0002352297306060791, 0.0002339780330657959]
I0108 21:37:08.499250 139932690765824 prune.py:307] recon error 0.0009874999523162842
I0108 21:37:09.244163 139932690765824 prune.py:207] pruning layer 7
I0108 21:37:11.019743 139932690765824 finetune.py:129] weight params, number of params: 7087872, weight_decay: 0.0, lr: 0.0002
I0108 21:37:20.559232 139932690765824 finetune.py:123] [0.0016219019889831543, 0.0007948875427246094, 0.0004798769950866699, 0.0004088282585144043, 0.0003746151924133301, 0.0003536343574523926, 0.00034058094024658203, 0.00033217668533325195, 0.0003253817558288574, 0.00032138824462890625, 0.000319063663482666]
I0108 21:37:21.330362 139932690765824 prune.py:307] recon error 0.0014677941799163818
I0108 21:37:22.069068 139932690765824 prune.py:207] pruning layer 8
I0108 21:37:23.831254 139932690765824 finetune.py:129] weight params, number of params: 7087872, weight_decay: 0.0, lr: 0.0002
I0108 21:37:33.311541 139932690765824 finetune.py:123] [0.002278059720993042, 0.001156926155090332, 0.0007339715957641602, 0.0006260871887207031, 0.0005718469619750977, 0.0005413293838500977, 0.0005221366882324219, 0.0005089044570922852, 0.0005004405975341797, 0.0004946589469909668, 0.0004916191101074219]
I0108 21:37:34.086050 139932690765824 prune.py:307] recon error 0.0023825764656066895
I0108 21:37:34.833746 139932690765824 prune.py:207] pruning layer 9
I0108 21:37:36.607979 139932690765824 finetune.py:129] weight params, number of params: 7087872, weight_decay: 0.0, lr: 0.0002
I0108 21:37:46.008382 139932690765824 finetune.py:123] [0.0029070377349853516, 0.0016932487487792969, 0.0011491775512695312, 0.0009968280792236328, 0.0009175539016723633, 0.0008709430694580078, 0.0008417367935180664, 0.0008211135864257812, 0.0008074045181274414, 0.0007985830307006836, 0.0007932186126708984]
I0108 21:37:46.775005 139932690765824 prune.py:307] recon error 0.004056394100189209
I0108 21:37:47.533107 139932690765824 prune.py:207] pruning layer 10
I0108 21:37:49.373002 139932690765824 finetune.py:129] weight params, number of params: 7087872, weight_decay: 0.0, lr: 0.0002
I0108 21:37:59.183499 139932690765824 finetune.py:123] [0.0039055943489074707, 0.0024056434631347656, 0.0016858577728271484, 0.0014684200286865234, 0.0013568401336669922, 0.001287698745727539, 0.00124359130859375, 0.0012145042419433594, 0.0011930465698242188, 0.001178741455078125, 0.0011696815490722656]
I0108 21:38:00.044578 139932690765824 prune.py:307] recon error 0.006733894348144531
I0108 21:38:00.807278 139932690765824 prune.py:207] pruning layer 11
I0108 21:38:02.589163 139932690765824 finetune.py:129] weight params, number of params: 7087872, weight_decay: 0.0, lr: 0.0002
I0108 21:38:12.149920 139932690765824 finetune.py:123] [0.005942225456237793, 0.0036802291870117188, 0.0024733543395996094, 0.002066373825073242, 0.0018699169158935547, 0.0017518997192382812, 0.001664876937866211, 0.0016083717346191406, 0.0015616416931152344, 0.001531362533569336, 0.001514434814453125]
I0108 21:38:12.923255 139932690765824 prune.py:307] recon error 0.01053309440612793
I0108 21:38:13.676807 139932690765824 opt.py:57] ******************************
I0108 21:38:13.681519 139932690765824 prune.py:56] block 0 sparsity 0.500061
I0108 21:38:13.684348 139932690765824 prune.py:56] block 1 sparsity 0.500110
I0108 21:38:13.687128 139932690765824 prune.py:56] block 2 sparsity 0.500103
I0108 21:38:13.691250 139932690765824 prune.py:56] block 3 sparsity 0.500110
I0108 21:38:13.694103 139932690765824 prune.py:56] block 4 sparsity 0.500143
I0108 21:38:13.696948 139932690765824 prune.py:56] block 5 sparsity 0.500164
I0108 21:38:13.699779 139932690765824 prune.py:56] block 6 sparsity 0.500264
I0108 21:38:13.702891 139932690765824 prune.py:56] block 7 sparsity 0.500186
I0108 21:38:13.706197 139932690765824 prune.py:56] block 8 sparsity 0.500085
I0108 21:38:13.710727 139932690765824 prune.py:56] block 9 sparsity 0.500065
I0108 21:38:13.712671 139932690765824 prune.py:56] block 10 sparsity 0.500237
I0108 21:38:13.714569 139932690765824 prune.py:56] block 11 sparsity 0.500227
I0108 21:38:13.714647 139932690765824 opt.py:59] sparsity sanity check 0.5001
I0108 21:38:13.714688 139932690765824 opt.py:60] ******************************
I0108 21:38:42.734274 139932690765824 eval.py:20] Evaluating ...
I0108 21:38:46.428000 139932690765824 eval.py:16] wikitext2 perplexity 36.03786087036133
I0108 21:38:56.829504 139932690765824 eval.py:20] Evaluating ...
I0108 21:38:58.179049 139932690765824 eval.py:16] ptb perplexity 49.190223693847656
